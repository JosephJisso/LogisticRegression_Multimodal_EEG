# -*- coding: utf-8 -*-
"""video_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b4tlZkJIf15GkpDHwzJq10dq2TpUYISZ
"""

import os
from google.colab import files

# 1. Upload your kaggle.json file
print("Please upload your kaggle.json file:")
files.upload()

# 2. Set up Kaggle directory and permissions
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# 3. Download and Unzip the dataset
# We use the exact dataset ID you provided
!kaggle datasets download -d ziya07/depvidmood-facial-expression-video-dataset
!unzip -q depvidmood-facial-expression-video-dataset.zip -d dataset

print("Dataset downloaded and extracted successfully!")



import cv2
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Define paths based on the structure you provided
BASE_DIR = "./dataset/Data/Depression Data/data"
TRAIN_DIR = os.path.join(BASE_DIR, "train")
TEST_DIR = os.path.join(BASE_DIR, "test")

# Categories usually linked to depression analysis
EMOTIONS = ["Angry", "Disgust", "Fear", "Happy", "Neutral", "Sad", "Surprize"]

def load_data(directory, img_size=48):
    data = []
    labels = []

    print(f"Loading data from {directory}...")

    for label in EMOTIONS:
        path = os.path.join(directory, label)
        if not os.path.exists(path):
            continue

        class_num = EMOTIONS.index(label)

        for img_name in os.listdir(path):
            try:
                # Read image in grayscale
                img_path = os.path.join(path, img_name)
                img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)

                # Resize to reduce feature count (Critical for Logistic Regression)
                new_array = cv2.resize(img_array, (img_size, img_size))

                # Flatten the image (2D -> 1D)
                data.append(new_array.flatten())
                labels.append(class_num)
            except Exception as e:
                pass

    return np.array(data), np.array(labels)

# Load Train and Test data
# We use a small image size (48x48) because Logistic Regression
# is slow with high-dimensional image data
X_train, y_train = load_data(TRAIN_DIR, img_size=48)
X_test, y_test = load_data(TEST_DIR, img_size=48)

# Scale the data (Normalization is crucial for Logistic Regression convergence)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Training Data Shape: {X_train_scaled.shape}")
print(f"Testing Data Shape: {X_test_scaled.shape}")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Train Logistic Regression
# max_iter is increased to ensure the model converges
print("Training Logistic Regression Model (this may take a moment)...")
log_reg = LogisticRegression(max_iter=2000, solver='lbfgs', multi_class='multinomial')
log_reg.fit(X_train_scaled, y_train)

# 2. Predictions
y_pred = log_reg.predict(X_test_scaled)

# 3. General Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.4f}")
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=EMOTIONS))

# 4. Confusion Matrix Visualization
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=EMOTIONS, yticklabels=EMOTIONS)
plt.title('Confusion Matrix - Emotion Recognition')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ---------------------------------------------------------
# 5. DEPRESSION ANALYSIS INTERPRETATION
# ---------------------------------------------------------
# In depression analysis, we often look at the ratio of
# Negative (Sad, Fear) vs Positive (Happy) expressions.

print("\n--- DEPRESSION ANALYSIS REPORT ---")

# Get indices for specific emotions
sad_idx = EMOTIONS.index("Sad")
fear_idx = EMOTIONS.index("Fear")
neutral_idx = EMOTIONS.index("Neutral")
happy_idx = EMOTIONS.index("Happy")

# Count predictions in Test Set
sad_count = np.sum(y_pred == sad_idx)
fear_count = np.sum(y_pred == fear_idx)
neutral_count = np.sum(y_pred == neutral_idx)
happy_count = np.sum(y_pred == happy_idx)
total_samples = len(y_pred)

print(f"Total Test Samples: {total_samples}")
print(f"Predicted 'Sad' Instances: {sad_count} ({(sad_count/total_samples)*100:.1f}%)")
print(f"Predicted 'Neutral' Instances: {neutral_count} ({(neutral_count/total_samples)*100:.1f}%)")
print(f"Predicted 'Happy' Instances: {happy_count} ({(happy_count/total_samples)*100:.1f}%)")

# A simple heuristic for 'Depressive Tendency' in this batch
# (High Sad/Neutral and Low Happy)
negative_load = sad_count + fear_count + neutral_count
positive_load = happy_count

ratio = negative_load / (positive_load + 1) # +1 to avoid division by zero

print(f"\nDepression Indicator Ratio (Neg/Pos): {ratio:.2f}")
if ratio > 2.0:
    print("Result: High prevalence of negative/flat affect detected in test batch. (Potential Depression Indicator)")
else:
    print("Result: Balanced or Positive affect detected.")

import cv2
import os
import numpy as np
import pandas as pd
from tqdm import tqdm  # Progress bar

# ---------------------------------------------------------
# 1. SETUP PATHS
# ---------------------------------------------------------
# Based on your structure: Video_Dataset -> Video_Speech_Actor_01 -> Actor_01 -> .mp4
BASE_VIDEO_DIR = "./dataset/Data/Video_Dataset/Video_Dataset"

# Define the Emotion Labels (Must match your training order)
EMOTIONS = ["Angry", "Disgust", "Fear", "Happy", "Neutral", "Sad", "Surprize"]

# ---------------------------------------------------------
# 2. DEFINE THE VIDEO ANALYZER FUNCTION
# ---------------------------------------------------------
def analyze_single_video(video_path, model, scaler):
    """
    Reads a video, predicts emotion for frames, and calculates depression risk.
    """
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        return None

    emotion_counts = {emotion: 0 for emotion in EMOTIONS}
    frame_count = 0
    total_frames_processed = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Optimization: Process only every 10th frame to save time
        if frame_count % 10 == 0:
            try:
                # Preprocessing to match Training Data
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)       # Convert to Gray
                resized = cv2.resize(gray, (48, 48))                 # Resize to 48x48
                flat = resized.flatten().reshape(1, -1)              # Flatten
                scaled_frame = scaler.transform(flat)                # Normalize

                # Predict
                prediction = model.predict(scaled_frame)[0]
                predicted_emotion = EMOTIONS[prediction]

                emotion_counts[predicted_emotion] += 1
                total_frames_processed += 1
            except Exception as e:
                pass # Skip bad frames

        frame_count += 1

    cap.release()

    if total_frames_processed == 0:
        return None

    # --- DEPRESSION HEURISTIC ---
    # Metric: Ratio of (Sad + Neutral + Fear) to (Happy)
    # High Ratio = Potential Depressive Affect (Flat or Negative)
    neg_score = emotion_counts["Sad"] + emotion_counts["Fear"] + emotion_counts["Neutral"]
    pos_score = emotion_counts["Happy"]

    # Avoid division by zero
    ratio = neg_score / (pos_score + 1)

    return {
        "video_name": os.path.basename(video_path),
        "total_frames": total_frames_processed,
        "sad_count": emotion_counts["Sad"],
        "neutral_count": emotion_counts["Neutral"],
        "happy_count": emotion_counts["Happy"],
        "depression_ratio": round(ratio, 2),
        "dominant_emotion": max(emotion_counts, key=emotion_counts.get)
    }

# ---------------------------------------------------------
# 3. MAIN LOOP: CRAWL FOLDERS AND ANALYZE
# ---------------------------------------------------------
results_list = []

print(f"Scanning directories in {BASE_VIDEO_DIR}...")

# Walk through the directory tree
for root, dirs, files in os.walk(BASE_VIDEO_DIR):
    for file in files:
        if file.endswith(".mp4"):
            video_path = os.path.join(root, file)

            # Optional: Limit to first 5 videos to save time during testing
            # if len(results_list) >= 5: break

            print(f"Analyzing {file}...")
            result = analyze_single_video(video_path, log_reg, scaler)

            if result:
                # Add 'Diagnosis' based on our ratio threshold
                if result['depression_ratio'] > 2.0:
                    result['assessment'] = "High Depressive Indicators"
                else:
                    result['assessment'] = "Normal/Balanced Affect"

                results_list.append(result)

# ---------------------------------------------------------
# 4. DISPLAY RESULTS
# ---------------------------------------------------------
if len(results_list) > 0:
    df_results = pd.DataFrame(results_list)

    # Reorder columns for readability
    cols = ["video_name", "assessment", "depression_ratio", "dominant_emotion", "sad_count", "happy_count"]
    df_results = df_results[cols]

    print("\n" + "="*50)
    print("FINAL DEPRESSION ANALYSIS REPORT")
    print("="*50)
    print(df_results.to_string(index=False))

    # Optional: Save to CSV
    df_results.to_csv("depression_analysis_results.csv", index=False)
    print("\nResults saved to 'depression_analysis_results.csv'")
else:
    print("No videos found or processed. Check your directory path.")

dep = pd.read_csv("depression_analysis_results.csv")
dep.head()

